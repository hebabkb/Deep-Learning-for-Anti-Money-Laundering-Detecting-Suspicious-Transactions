{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"berkanoztas/synthetic-transaction-monitoring-dataset-aml\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1m-e6VliGxnN",
        "outputId": "2ad1592c-690f-415f-be44-0fc80aef7539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/berkanoztas/synthetic-transaction-monitoring-dataset-aml?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 193M/193M [00:07<00:00, 26.4MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/berkanoztas/synthetic-transaction-monitoring-dataset-aml/versions/2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hThx_lLRGi9N"
      },
      "outputs": [],
      "source": [
        "data=pd.read_csv(\"/root/.cache/kagglehub/datasets/berkanoztas/synthetic-transaction-monitoring-dataset-aml/versions/2/SAML-D.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "\n",
        "\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "data['Time'] = pd.to_timedelta(data['Time'])\n",
        "\n",
        "\n",
        "\n",
        "def sample_data(df, validation_dt=70, test_dt=40):\n",
        "  # validation_cut_off=70: selects transactions for validation set, from the final 70 days prior to the dataset's latest date\n",
        "  # test_cut_off=40: selects transactions for test set, from the final 70 days prior to the dataset's latest date\n",
        "\n",
        "  test_cutoff = df['Date'].max() - pd.Timedelta(days=test_dt)\n",
        "  validation_cutoff = df['Date'].max() - pd.Timedelta(days=validation_dt)\n",
        "\n",
        "  test_size = len(df[df.Date >= test_cutoff])\n",
        "  validation_size = len(df[(df.Date >= validation_cutoff) & (df.Date < test_cutoff)])\n",
        "  train_size = len(df[df.Date < validation_cutoff])\n",
        "\n",
        "  test_percentage = test_size/len(df)\n",
        "  validation_percentage = validation_size/len(df)\n",
        "  train_percentage = train_size/len(df)\n",
        "\n",
        "  print(f\"Test size: {test_size} ({test_percentage*100:.2f}%)\")\n",
        "  print(f\"Validation size: {validation_size} ({validation_percentage*100:.2f}%)\")\n",
        "  print(f\"Train size: {train_size} ({train_percentage*100:.2f}%)\")\n",
        "\n",
        "  test_laundering = df[df.Date >= test_cutoff].Is_laundering.sum()/len(df[df.Date >= test_cutoff])\n",
        "  validation_laundering = df[(df.Date >= validation_cutoff) & (df.Date < test_cutoff)].Is_laundering.sum()/len(df[(df.Date >= validation_cutoff) & (df.Date < test_cutoff)])\n",
        "  train_laundering = df[df.Date < validation_cutoff].Is_laundering.sum()/len(df[df.Date < validation_cutoff])\n",
        "\n",
        "  train = df[df.Date < validation_cutoff]\n",
        "  val = df[(df.Date >= validation_cutoff) & (df.Date < test_cutoff)]\n",
        "  test = df[df.Date >= test_cutoff]\n",
        "\n",
        "  train.to_csv(\"train.csv\")\n",
        "  val.to_csv(\"validation.csv\")\n",
        "  test.to_csv(\"test.csv\")\n",
        "\n",
        "  print(f\"Laundering proportion (Test set): {test_laundering}\")\n",
        "  print(f\"Laundering proportion (Validation set): {validation_laundering}\")\n",
        "  print(f\"Laundering proportion (Train set): {train_laundering}\")"
      ],
      "metadata": {
        "id": "Br-VhnZlkuiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_data(data, 70, 35)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2WJlB39k5Yz",
        "outputId": "65ac66bd-9227-45cd-d2b0-6deee1d682ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test size: 1030388 (10.84%)\n",
            "Validation size: 1044845 (10.99%)\n",
            "Train size: 7429619 (78.17%)\n",
            "Laundering proportion (Test set): 0.0011626688199008529\n",
            "Laundering proportion (Validation set): 0.0011446673908570171\n",
            "Laundering proportion (Train set): 0.0010066465050226666\n"
          ]
        }
      ]
    }
  ]
}