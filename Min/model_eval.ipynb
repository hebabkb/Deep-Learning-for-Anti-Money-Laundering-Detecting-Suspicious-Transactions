{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50bf8718",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple, List\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a1a46d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smile\\AppData\\Local\\Temp\\ipykernel_20284\\3953609690.py:307: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model_weights.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7429619, 14]) torch.int64\n",
      "torch.Size([7429619, 1]) torch.float32\n",
      "torch.Size([7429619]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "categorical_cols = ['hour', 'day_of_week', 'is_weekend', 'day', 'year', 'month', 'week',\n",
    "                    'Sender_account', 'Receiver_account',\n",
    "                    'Payment_currency', 'Received_currency',\n",
    "                    'Sender_bank_location', 'Receiver_bank_location', 'Payment_type']\n",
    "continuous_cols = [\"Amount\"]\n",
    "target_col = 'Is_laundering'\n",
    "\n",
    "train = pd.read_csv(\"preprocessed_train.csv\")[categorical_cols + continuous_cols + [target_col]]\n",
    "val = pd.read_csv(\"preprocessed_validation.csv\")[categorical_cols + continuous_cols + [target_col]]\n",
    "test = pd.read_csv(\"preprocessed_test.csv\")[categorical_cols + continuous_cols + [target_col]]\n",
    "\n",
    "\n",
    "for df in [train, val, test]:\n",
    "    df[\"is_weekend\"] = df[\"is_weekend\"].astype(int)\n",
    "num_categories = []\n",
    "\n",
    "for col in categorical_cols:\n",
    "    num_categories.append(train[col].nunique())\n",
    "num_categories[5] = 12\n",
    "num_categories[6] = 52\n",
    "class TabularEmbedding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_categories,  # list of cardinalities for each categorical feature\n",
    "                 num_continuous,  # number of continuous features\n",
    "                 d_model):        # transformer model dimension\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_categorical = len(num_categories)\n",
    "        self.num_continuous = num_continuous\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # --- Categorical embeddings (column-based) ---\n",
    "        self.cat_embeddings = nn.ModuleList([\n",
    "            nn.Embedding(num_embeddings=cardinality, embedding_dim=d_model)\n",
    "            for cardinality in num_categories\n",
    "        ])\n",
    "\n",
    "        # Initialize with truncated normal (mean=0, std=0.01)\n",
    "        for emb in self.cat_embeddings:\n",
    "            nn.init.trunc_normal_(emb.weight, mean=0.0, std=0.01)\n",
    "\n",
    "        # --- Continuous projection layer ---\n",
    "        # Project all continuous features into d_model\n",
    "        if num_continuous > 0:\n",
    "            self.continuous_proj = nn.Linear(num_continuous, d_model)\n",
    "            nn.init.trunc_normal_(self.continuous_proj.weight, mean=0.0, std=0.01)\n",
    "            nn.init.zeros_(self.continuous_proj.bias)\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        \"\"\"\n",
    "        x_cat: [batch_size, num_categorical]  (long tensor)\n",
    "        x_cont: [batch_size, num_continuous]  (float tensor)\n",
    "        \"\"\"\n",
    "        # Embed categorical columns\n",
    "        cat_embs = [emb(x_cat[:, i]) for i, emb in enumerate(self.cat_embeddings)]\n",
    "        cat_embs = torch.stack(cat_embs, dim=1)  # [batch_size, num_categorical, d_model]\n",
    "\n",
    "        # Process continuous features (if any)\n",
    "        if self.num_continuous > 0:\n",
    "            cont_emb = self.continuous_proj(x_cont).unsqueeze(1)  # [batch_size, 1, d_model]\n",
    "            x = torch.cat([cat_embs, cont_emb], dim=1)  # total_tokens = num_categorical + 1\n",
    "        else:\n",
    "            x = cat_embs\n",
    "\n",
    "        return x  # [batch_size, num_tokens, d_model]\n",
    "\n",
    "class ResidualAttentionEncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Single transformer encoder layer with:\n",
    "      - multi-head self-attention\n",
    "      - residual attention integration: attn_out + alpha * prev_attn_out\n",
    "      - residual connection + dropout + layernorm\n",
    "      - feed-forward MLP (GeLU)\n",
    "      - returns (out, attn_out) where attn_out can be passed to next layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 nhead: int,\n",
    "                 dim_feedforward: int = 2048,\n",
    "                 dropout: float = 0.1,\n",
    "                 attn_dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, dropout=attn_dropout, batch_first=True)\n",
    "\n",
    "        # Learnable scalar for residual-attention gating\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.0))  # initialize near 0, model learns whether to use it\n",
    "\n",
    "        # Residual dropout\n",
    "        self.residual_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # LayerNorms and FFN\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, d_model),\n",
    "        )\n",
    "\n",
    "        # final dropout on FFN output (already included inside ffn)\n",
    "        self.final_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                attn_mask: Optional[torch.Tensor] = None,\n",
    "                key_padding_mask: Optional[torch.Tensor] = None,\n",
    "                prev_attn_out: Optional[torch.Tensor] = None\n",
    "                ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        x: [batch, seq_len, d_model]\n",
    "        attn_mask: optional causal or custom mask (seq_len, seq_len) or (batch, seq_len, seq_len) (PyTorch handles various shapes)\n",
    "        key_padding_mask: optional (batch, seq_len) boolean mask for padding\n",
    "        prev_attn_out: previous layer's attention output (same shape as attn_out) OR None\n",
    "        returns: (out, attn_out)\n",
    "        \"\"\"\n",
    "\n",
    "        # MultiheadAttention in batch_first mode: returns (attn_output, attn_weights)\n",
    "        attn_out, attn_weights = self.self_attn(query=x,\n",
    "                                               key=x,\n",
    "                                               value=x,\n",
    "                                               attn_mask=attn_mask,\n",
    "                                               key_padding_mask=key_padding_mask,\n",
    "                                               need_weights=True,\n",
    "                                               average_attn_weights=False)  # attn_out: [batch, seq_len, d_model]\n",
    "        # attn_weights shape: [batch, nhead, seq_len, seq_len]\n",
    "\n",
    "        # Optionally integrate residual attention (previous layer's attn_out)\n",
    "        if prev_attn_out is not None:\n",
    "            # prev_attn_out is expected to be same shape as attn_out: [batch, seq_len, d_model]\n",
    "            attn_out = attn_out + self.alpha * prev_attn_out\n",
    "\n",
    "        # Residual + LayerNorm\n",
    "        x = x + self.residual_dropout(attn_out)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # Feed-forward\n",
    "        ffn_out = self.ffn(x)               # [batch, seq_len, d_model]\n",
    "        x = x + self.final_dropout(ffn_out)\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        # Return the post-attention vector as the layer's 'attention output' for residual-attention chaining\n",
    "        # We return attn_out (before residual-sum into x) so next layer can combine it\n",
    "        return x, attn_out\n",
    "\n",
    "\n",
    "class MaskedEncoderStack(nn.Module):\n",
    "    \"\"\"\n",
    "    Stack of ResidualAttentionEncoderLayer that runs on a *subset* of tokens\n",
    "    (e.g., only sender+receiver tokens). We pass prev_attn between layers to enable\n",
    "    residual-attention chaining.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_layers: int, **layer_kwargs):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([ResidualAttentionEncoderLayer(**layer_kwargs) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None, key_padding_mask: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        x: [batch, seq_len_subset, d_model]  (e.g., seq_len_subset==2 for sender+receiver)\n",
    "        returns: output of stack, plus last layer's attn_out (so caller can optionally inspect)\n",
    "        \"\"\"\n",
    "        prev_attn = None\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out, prev_attn = layer(out, attn_mask=attn_mask, key_padding_mask=key_padding_mask, prev_attn_out=prev_attn)\n",
    "        return out, prev_attn\n",
    "\n",
    "\n",
    "class FullEncoderStack(nn.Module):\n",
    "    \"\"\"\n",
    "    Stack of ResidualAttentionEncoderLayer that runs on the full token sequence (unmasked)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_layers: int, **layer_kwargs):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([ResidualAttentionEncoderLayer(**layer_kwargs) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None, key_padding_mask: Optional[torch.Tensor] = None):\n",
    "        prev_attn = None\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out, prev_attn = layer(out, attn_mask=attn_mask, key_padding_mask=key_padding_mask, prev_attn_out=prev_attn)\n",
    "        return out, prev_attn\n",
    "\n",
    "\n",
    "class DualMaskedTransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the two-stage encoder:\n",
    "     1) Masked encoder on subset tokens (sender+receiver)\n",
    "        - we extract the two tokens, run the masked stack, then reinsert updated embeddings\n",
    "     2) Full encoder on all tokens\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 nhead: int,\n",
    "                 n_layers_masked: int = 2,\n",
    "                 n_layers_full: int = 2,\n",
    "                 dim_feedforward: int = 2048,\n",
    "                 dropout: float = 0.1,\n",
    "                 attn_dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        layer_kwargs = dict(d_model=d_model,\n",
    "                            nhead=nhead,\n",
    "                            dim_feedforward=dim_feedforward,\n",
    "                            dropout=dropout,\n",
    "                            attn_dropout=attn_dropout)\n",
    "\n",
    "        self.masked_encoder = MaskedEncoderStack(n_layers=n_layers_masked, **layer_kwargs)\n",
    "        self.full_encoder = FullEncoderStack(n_layers=n_layers_full, **layer_kwargs)\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                sender_idx: int,\n",
    "                receiver_idx: int,\n",
    "                key_padding_mask: Optional[torch.Tensor] = None\n",
    "                ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: [batch, seq_len, d_model]\n",
    "        sender_idx, receiver_idx: integer token indices (same for all samples in batch)\n",
    "            - If token positions are different per example, you'd pass per-sample indices and use gather.\n",
    "        key_padding_mask: optional (batch, seq_len) boolean mask\n",
    "        returns: output after full encoder: [batch, seq_len, d_model]\n",
    "        \"\"\"\n",
    "\n",
    "        batch, seq_len, d_model = x.shape\n",
    "        assert d_model == self.full_encoder.layers[0].d_model\n",
    "\n",
    "        # 1) Extract sender & receiver tokens as a small sequence [batch, 2, d_model]\n",
    "        pair_indices = torch.tensor([sender_idx, receiver_idx], device=x.device)\n",
    "        # For simplicity we assume same indices for all samples (common in tabular pipelines)\n",
    "        x_pairs = x[:, pair_indices, :]  # [batch, 2, d_model]\n",
    "\n",
    "        # (Optional) We could build a small attention mask for the pair (e.g., allow mutual attention) - not needed here\n",
    "        masked_out, last_pair_attn = self.masked_encoder(x_pairs)  # [batch, 2, d_model]\n",
    "\n",
    "        # 2) Reinsert updated pair embeddings back into the full sequence\n",
    "        x_updated = x.clone()\n",
    "        x_updated[:, sender_idx, :] = masked_out[:, 0, :]\n",
    "        x_updated[:, receiver_idx, :] = masked_out[:, 1, :]\n",
    "\n",
    "        # 3) Run full encoder (unmasked) to integrate pair features into context\n",
    "        full_out, last_full_attn = self.full_encoder(x_updated, key_padding_mask=key_padding_mask)\n",
    "\n",
    "        return full_out\n",
    "\n",
    "d_model = 64\n",
    "\n",
    "embedding_layer = TabularEmbedding(\n",
    "    num_categories=num_categories,\n",
    "    num_continuous=len(continuous_cols),\n",
    "    d_model=d_model\n",
    ")\n",
    "\n",
    "transformer = DualMaskedTransformerEncoder(\n",
    "    d_model=d_model,\n",
    "    nhead=8,\n",
    "    n_layers_masked=2,\n",
    "    n_layers_full=2,\n",
    "    dim_feedforward=256,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# simple classifier head\n",
    "class ClassifierHead(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_model//2, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # Use mean pooling over sequence dimension\n",
    "        x = x.mean(dim=1)\n",
    "        print(x.shape)\n",
    "        return self.fc(x).squeeze(-1)\n",
    "\n",
    "classifier = ClassifierHead(d_model)\n",
    "class TabAMLModel(nn.Module):\n",
    "    def __init__(self, embedding_layer, transformer, classifier,\n",
    "                 sender_idx=0, receiver_idx=1):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding_layer\n",
    "        self.transformer = transformer\n",
    "        self.classifier = classifier\n",
    "        self.sender_idx = sender_idx\n",
    "        self.receiver_idx = receiver_idx\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        # Step 1: embed both categorical and continuous features\n",
    "        x = self.embedding(x_cat, x_cont)  # [batch, num_tokens, d_model]\n",
    "        # Step 2: run dual transformer\n",
    "        x = self.transformer(x, sender_idx=self.sender_idx, receiver_idx=self.receiver_idx)\n",
    "        # Step 3: predict\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "model = TabAMLModel(embedding_layer, transformer, classifier)\n",
    "model.load_state_dict(torch.load(\"model_weights.pth\"))\n",
    "\n",
    "for col in categorical_cols:\n",
    "    # Make sure no negative or non-integer values\n",
    "    if train[col].dtype == 'int' or np.issubdtype(train[col].dtype, np.integer):\n",
    "        train[col] -= train[col].min()\n",
    "        val[col] -= val[col].min()\n",
    "        test[col] -= test[col].min()\n",
    "X_cat_train = torch.tensor(train[categorical_cols].values, dtype=torch.long)\n",
    "X_cont_train = torch.tensor(train[continuous_cols].values, dtype=torch.float)\n",
    "y_train = torch.tensor(train[target_col].values, dtype=torch.float)\n",
    "\n",
    "X_cat_val = torch.tensor(val[categorical_cols].values, dtype=torch.long)\n",
    "X_cont_val = torch.tensor(val[continuous_cols].values, dtype=torch.float)\n",
    "y_val = torch.tensor(val[target_col].values, dtype=torch.float)\n",
    "\n",
    "X_cat_test = torch.tensor(test[categorical_cols].values, dtype=torch.long)\n",
    "X_cont_test = torch.tensor(test[continuous_cols].values, dtype=torch.float)\n",
    "y_test = torch.tensor(test[target_col].values, dtype=torch.float)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "for i, col in enumerate(categorical_cols):\n",
    "    max_idx = X_cat_train[:, i].max().item()\n",
    "    min_idx = X_cat_train[:, i].min().item()\n",
    "    if max_idx >= num_categories[i] or min_idx < 0:\n",
    "        print(f\"❌ Column '{col}' has out-of-range index [{min_idx}, {max_idx}] \"\n",
    "              f\"but embedding size is {num_categories[i]}\")\n",
    "print(X_cat_train.shape, X_cat_train.dtype)\n",
    "print(X_cont_train.shape, X_cont_train.dtype)\n",
    "print(y_train.shape, y_train.dtype)\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_cat_train, X_cont_train, y_train)\n",
    "val_dataset = TensorDataset(X_cat_val, X_cont_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0f451d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([1024, 64])\n",
      "torch.Size([244, 64])\n",
      "Accuracy:  0.978\n",
      "Recall:    0.420\n",
      "Precision: 0.022\n",
      "F1 Score:  0.042\n"
     ]
    }
   ],
   "source": [
    "# Use DataLoader to handle batching\n",
    "batch_size = 1024  # try 512 if memory is still low\n",
    "test_dataset = TensorDataset(X_cat_test, X_cont_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "all_logits = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_cat_batch, X_cont_batch, _ in test_loader:\n",
    "        logits = model(X_cat_batch, X_cont_batch)\n",
    "        all_logits.append(logits.cpu())\n",
    "\n",
    "# Concatenate all batch outputs\n",
    "logits = torch.cat(all_logits)\n",
    "probs = torch.sigmoid(logits)\n",
    "preds = (probs > 0.5).float()\n",
    "\n",
    "# Convert tensors to numpy arrays\n",
    "y_true = y_test.cpu().numpy()\n",
    "y_pred = preds.cpu().numpy()\n",
    "\n",
    "# Compute metrics\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Accuracy:  {acc:.3f}\")\n",
    "print(f\"Recall:    {recall:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"F1 Score:  {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f3e1928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[1006802   22388]\n",
      " [    695     503]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_summer_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
