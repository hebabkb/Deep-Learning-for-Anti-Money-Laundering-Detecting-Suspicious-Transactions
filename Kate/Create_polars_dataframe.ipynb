{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Create Polars dataframe**\n",
        "\n",
        "Custom split data into Train, Validation, and Spilt, as well as adding additional features. Save those in Google Drive"
      ],
      "metadata": {
        "id": "6C8MFs-dBxen"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set up**"
      ],
      "metadata": {
        "id": "bJ6UCDPZCupB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "BldEjuw1Bmtp",
        "outputId": "42a6c738-b75e-4aef-d7f3-18328e5cc695"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5bd0445c-f2ce-42af-a89f-13f353653cfb\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5bd0445c-f2ce-42af-a89f-13f353653cfb\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"phuongkhanh21\",\"key\":\"0fd6eb70e1509aad441adeb76f0dfff2\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "drive_path = '/content/drive/MyDrive/Colab Notebooks/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGBbjNnDCGW6",
        "outputId": "d886b9ed-0c4a-4492-948a-91018df391b4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "!mv kaggle.json /root/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle datasets download -d berkanoztas/synthetic-transaction-monitoring-dataset-aml\n",
        "\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"synthetic-transaction-monitoring-dataset-aml.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"synthetic_transaction_data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hg4_8UXsCIuV",
        "outputId": "07cbb030-a84e-45ed-fa1e-0736409f0dd4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/berkanoztas/synthetic-transaction-monitoring-dataset-aml\n",
            "License(s): CC-BY-NC-SA-4.0\n",
            "synthetic-transaction-monitoring-dataset-aml.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import libraries**"
      ],
      "metadata": {
        "id": "SiT4IG-gCzqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rustworkx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDc3b554CJMa",
        "outputId": "fb1268c3-1f06-403a-9275-50b09f02fb56"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rustworkx in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
            "Requirement already satisfied: numpy<3,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from rustworkx) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import polars as pl\n",
        "from datetime import timedelta\n",
        "import numpy as np\n",
        "import rustworkx as rx\n",
        "from typing import List, Dict"
      ],
      "metadata": {
        "id": "sBkb6wnnCLEc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Custom split data in polars**"
      ],
      "metadata": {
        "id": "2gdgDG3hC4HU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_split_polars(df: pl.DataFrame, validation_days: int = 70, test_days: int = 35):\n",
        "      \"\"\"\n",
        "      Chronological split (train → val → test) based on calendar days from earliest to latest.\n",
        "      Ensures non-overlapping sequential windows.\n",
        "      \"\"\"\n",
        "      # Convert to datetime if needed\n",
        "      dtype = df[\"Date\"].dtype\n",
        "      if dtype == pl.Utf8:\n",
        "          df = df.with_columns(pl.col(\"Date\").str.strptime(pl.Datetime(), \"%Y-%m-%d\", strict=False))\n",
        "      elif dtype == pl.Date:\n",
        "          df = df.with_columns(pl.col(\"Date\").cast(pl.Datetime()))\n",
        "      df = df.sort(\"Date\")\n",
        "      min_date = df.select(pl.col(\"Date\").min()).item()\n",
        "      max_date = df.select(pl.col(\"Date\").max()).item()\n",
        "      total_days = (max_date - min_date).days\n",
        "      # Split: oldest → newest\n",
        "      test_cutoff = max_date - timedelta(days=test_days)\n",
        "      val_cutoff = test_cutoff - timedelta(days=validation_days)\n",
        "      train_df = df.filter(pl.col(\"Date\") < pl.lit(val_cutoff))\n",
        "      val_df   = df.filter((pl.col(\"Date\") >= pl.lit(val_cutoff)) & (pl.col(\"Date\") < pl.lit(test_cutoff)))\n",
        "      test_df  = df.filter(pl.col(\"Date\") >= pl.lit(test_cutoff))\n",
        "      print(f\"Split complete:\")\n",
        "      print(f\"  Train: {train_df.height} rows (until {val_cutoff.date()})\")\n",
        "      print(f\"  Val:   {val_df.height} rows ({val_cutoff.date()}–{test_cutoff.date()})\")\n",
        "      print(f\"  Test:  {test_df.height} rows (after {test_cutoff.date()})\")\n",
        "      return train_df, val_df, test_df"
      ],
      "metadata": {
        "id": "JTFl806nCOnF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Engineering Features**"
      ],
      "metadata": {
        "id": "MXY3bHsRC8J8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_engineering(df: pl.DataFrame) -> pl.DataFrame:\n",
        "    \"\"\"\n",
        "    Build and return engineered features from a Polars DataFrame.\n",
        "    \"\"\"\n",
        "\n",
        "    # Dropping column Laundering_type\n",
        "    df = df.drop(\"Laundering_type\")\n",
        "\n",
        "    # Temporal features\n",
        "    def temporal_features(df):\n",
        "\n",
        "        return df.with_columns([\n",
        "            df[\"Date\"].dt.year().alias(\"year\"),\n",
        "            df[\"Date\"].dt.month().alias(\"month\"),\n",
        "            df[\"Date\"].dt.day().alias(\"day_of_month\"),\n",
        "            df[\"Date\"].dt.weekday().alias(\"day_of_week\"),\n",
        "            df[\"Date\"].dt.ordinal_day().alias(\"day_of_year\"),\n",
        "            df[\"Time\"].dt.hour().alias(\"hour\"),\n",
        "            df[\"Time\"].dt.minute().alias(\"minute\"),\n",
        "            df[\"Time\"].dt.second().alias(\"second\"),\n",
        "        ])\n",
        "\n",
        "    # Risk features\n",
        "    high_risk_countries = ['Mexico', 'Turkey', 'Morocco', 'UAE']\n",
        "\n",
        "    def risk_features(df):\n",
        "\n",
        "        return df.with_columns([\n",
        "            (df[\"Payment_currency\"] != df[\"Received_currency\"]).cast(pl.Int8).alias(\"currency_mismatch\"),\n",
        "            (df[\"Payment_type\"] == \"Cross-border\").cast(pl.Int8).alias(\"cross_border\"),\n",
        "            df[\"Sender_bank_location\"].is_in(high_risk_countries).cast(pl.Int8).alias(\"high_risk_sender\"),\n",
        "            df[\"Receiver_bank_location\"].is_in(high_risk_countries).cast(pl.Int8).alias(\"high_risk_receiver\")])\n",
        "\n",
        "    def build_window_features_lazy(\n",
        "        df,\n",
        "        specs,\n",
        "        date_col=\"Date\",\n",
        "        sender_col=\"Sender_account\",\n",
        "        receiver_col=\"Receiver_account\",\n",
        "        amount_col=\"Amount\",\n",
        "        index_name=\"__row_idx\",\n",
        "        label_choice=\"left\",\n",
        "    ):\n",
        "        lf = df.lazy() if isinstance(df, pl.DataFrame) else df\n",
        "        lf = lf.with_columns(pl.arange(0, pl.len()).over(pl.lit(True)).alias(index_name))\n",
        "        out_lf = lf\n",
        "\n",
        "        for spec in specs:\n",
        "            kind = spec.get(\"kind\", \"rolling\")\n",
        "\n",
        "            if kind == \"rolling\":\n",
        "                # existing rolling logic (no change)\n",
        "                name = spec[\"name\"]\n",
        "                direction = spec[\"type\"]  # \"fanin\" or \"fanout\"\n",
        "                period_days = int(spec[\"period_days\"])\n",
        "                every = spec.get(\"every\", \"1d\")\n",
        "\n",
        "                if direction == \"fanin\":\n",
        "                    group_by = receiver_col\n",
        "                    agg_on = sender_col\n",
        "                else:\n",
        "                    group_by = sender_col\n",
        "                    agg_on = receiver_col\n",
        "\n",
        "                win_label = label_choice\n",
        "                strategy = \"forward\" if win_label == \"left\" else \"backward\"\n",
        "\n",
        "                right = (\n",
        "                    lf\n",
        "                    .sort([group_by, date_col])\n",
        "                    .group_by_dynamic(\n",
        "                        index_column=date_col,\n",
        "                        every=every,\n",
        "                        period=f\"{period_days}d\",\n",
        "                        group_by=group_by,\n",
        "                        closed=\"both\",\n",
        "                        label=win_label\n",
        "                    )\n",
        "                    .agg(pl.col(agg_on).n_unique().alias(name))\n",
        "                    .sort([group_by, date_col])\n",
        "                )\n",
        "\n",
        "                left = out_lf.sort([group_by, date_col])\n",
        "\n",
        "                out_lf = left.join_asof(\n",
        "                    right,\n",
        "                    left_on=date_col,\n",
        "                    right_on=date_col,\n",
        "                    by=group_by,\n",
        "                    strategy=strategy,\n",
        "                )\n",
        "\n",
        "            elif kind == \"monthly\":\n",
        "                # existing monthly logic (no change)\n",
        "                name = spec[\"name\"]\n",
        "                side = spec.get(\"side\", \"receive\")\n",
        "                group_col = receiver_col if side == \"receive\" else sender_col\n",
        "\n",
        "                monthly_agg = (\n",
        "                    lf\n",
        "                    .with_columns(pl.col(date_col).dt.truncate(\"1mo\").alias(\"__month\"))\n",
        "                    .group_by([group_col, \"__month\"])\n",
        "                    .agg(pl.col(amount_col).sum().alias(name))\n",
        "                )\n",
        "\n",
        "                out_lf = (\n",
        "                    out_lf\n",
        "                    .with_columns(pl.col(date_col).dt.truncate(\"1mo\").alias(\"__month\"))\n",
        "                    .join(monthly_agg, on=[group_col, \"__month\"], how=\"left\")\n",
        "                    .drop(\"__month\")\n",
        "                )\n",
        "\n",
        "            elif kind == \"daily_pair_count\":\n",
        "                # NEW: back_and_forth_transfers (exact-match on day + pair)\n",
        "                name = spec[\"name\"]  # e.g., \"back_and_forth_transfers\"\n",
        "                # day key = calendar day (truncate to 1 day)\n",
        "                day_key = \"__day\"\n",
        "                # compute counts per sender/receiver/day using lf (lazy)\n",
        "                pair_daily_agg = (\n",
        "                    lf\n",
        "                    .with_columns(pl.col(date_col).dt.truncate(\"1d\").alias(day_key))\n",
        "                    .group_by([sender_col, receiver_col, day_key])\n",
        "                    .agg(pl.len().alias(name))  # .len() counts rows in group\n",
        "                )\n",
        "\n",
        "                # attach day key to working frame and join exact on pair + day\n",
        "                out_lf = (\n",
        "                    out_lf\n",
        "                    .with_columns(pl.col(date_col).dt.truncate(\"1d\").alias(day_key))\n",
        "                    .join(pair_daily_agg, on=[sender_col, receiver_col, day_key], how=\"left\")\n",
        "                    .fill_null(0)       # optional: replace nulls with 0\n",
        "                    .with_columns(pl.col(name).cast(pl.Int64))  # ensure integer type\n",
        "                    .drop(day_key)\n",
        "                )\n",
        "\n",
        "            else:\n",
        "                raise ValueError(\"spec kind must be 'rolling', 'monthly', or 'daily_pair_count'\")\n",
        "\n",
        "        return out_lf\n",
        "\n",
        "\n",
        "    def compute_derived_features_lazy(\n",
        "        lf: pl.LazyFrame,\n",
        "        *,\n",
        "        fanin_col: str = \"fanin_30d\",\n",
        "        fanout_col: str = \"fanout_30d\",\n",
        "        daily_receive_col: str = \"daily_receive\",\n",
        "        monthly_receive_col: str = \"monthly_receive\",\n",
        "        monthly_send_col: str = \"monthly_send\",\n",
        "        amount_col: str = \"Amount\",\n",
        "        sender_col: str = \"Sender_account\",\n",
        "        receiver_col: str = \"Receiver_account\",\n",
        "        index_name: str = \"__row_idx\",\n",
        "    ) -> pl.LazyFrame:\n",
        "        \"\"\"\n",
        "        Take a LazyFrame and return a LazyFrame with derived features:\n",
        "          - fan_in_out_ratio (safe division, 0 when denom missing or zero)\n",
        "          - fanin_intensity_ratio (fanin_30d / daily_receive, 0 when denom missing or zero)\n",
        "          - amount_dispersion_std (per-sender std of Amount, filled 0 when null)\n",
        "          - sent_to_received_ratio_monthly (monthly_receive / monthly_send, 0 when denom missing or zero)\n",
        "\n",
        "        If `daily_receive` is not present in lf.schema(), it is computed lazily as the\n",
        "        per-receiver unique-senders per calendar day (dt.truncate(\"1d\")) and joined back.\n",
        "        The function is fully lazy; call .collect(...) when ready.\n",
        "        \"\"\"\n",
        "        # ensure lazy input\n",
        "        lf = lf if isinstance(lf, pl.LazyFrame) else lf.lazy()\n",
        "\n",
        "        # Attempt to read schema; if unavailable assume missing and compute\n",
        "        try:\n",
        "            schema = lf.schema()\n",
        "            has_daily = daily_receive_col in schema\n",
        "        except Exception:\n",
        "            has_daily = False\n",
        "\n",
        "        # If daily_receive missing, compute it lazily (exact day bucket of unique senders per receiver)\n",
        "        if not has_daily:\n",
        "            day_key = \"__day_for_daily_receive\"\n",
        "            daily_receive_agg = (\n",
        "                lf\n",
        "                .with_columns(pl.col(\"Date\").dt.truncate(\"1d\").alias(day_key))\n",
        "                .group_by([receiver_col, day_key])\n",
        "                .agg(pl.col(sender_col).n_unique().alias(daily_receive_col))\n",
        "            )\n",
        "            lf = (\n",
        "                lf\n",
        "                .with_columns(pl.col(\"Date\").dt.truncate(\"1d\").alias(day_key))\n",
        "                .join(daily_receive_agg, on=[receiver_col, day_key], how=\"left\")\n",
        "                .drop(day_key)\n",
        "            )\n",
        "\n",
        "        # safe division helper expression\n",
        "        def safe_div_expr(num: str, den: str, out_name: str):\n",
        "            return (\n",
        "                pl.when(pl.col(den).is_null() | (pl.col(den) == 0))\n",
        "                  .then(0.0)\n",
        "                  .otherwise(pl.col(num).cast(pl.Float64) / pl.col(den).cast(pl.Float64))\n",
        "                  .alias(out_name)\n",
        "            )\n",
        "\n",
        "        fan_in_out_expr = safe_div_expr(fanin_col, fanout_col, \"fan_in_out_ratio\")\n",
        "        fanin_intensity_expr = safe_div_expr(fanin_col, daily_receive_col, \"fanin_intensity_ratio\")\n",
        "        sent_to_received_monthly_expr = safe_div_expr(monthly_receive_col, monthly_send_col, \"sent_to_received_ratio_monthly\")\n",
        "\n",
        "        # per-sender std aggregation (lazy) and join back\n",
        "        sender_std_agg = (\n",
        "            lf\n",
        "            .select([sender_col, amount_col])\n",
        "            .group_by(sender_col)\n",
        "            .agg(pl.col(amount_col).std().alias(\"__amount_std\"))\n",
        "        )\n",
        "\n",
        "        out = (\n",
        "            lf\n",
        "            .join(sender_std_agg, on=sender_col, how=\"left\")\n",
        "            .with_columns(\n",
        "                pl.col(\"__amount_std\").cast(pl.Float64).fill_null(0.0).alias(\"amount_dispersion_std\")\n",
        "            )\n",
        "            .drop(\"__amount_std\")\n",
        "            .with_columns([\n",
        "                fan_in_out_expr,\n",
        "                fanin_intensity_expr,\n",
        "                sent_to_received_monthly_expr\n",
        "            ])\n",
        "        )\n",
        "\n",
        "        return out\n",
        "    # Temporal features\n",
        "    df = temporal_features(df)\n",
        "\n",
        "    # Risk features\n",
        "    df = risk_features(df)\n",
        "\n",
        "    # Rolling window computing\n",
        "    specs = [\n",
        "    {\"name\":\"fanin_30d\", \"kind\":\"rolling\", \"type\":\"fanin\", \"period_days\":30, \"every\":\"1d\"},\n",
        "    {\"name\":\"fanout_30d\", \"kind\":\"rolling\", \"type\":\"fanout\", \"period_days\":30, \"every\":\"1d\"},\n",
        "    {\"name\":\"daily_recieve\", \"kind\":\"rolling\", \"type\":\"fanin\", \"period_days\":1, \"every\":\"1d\"},\n",
        "    {\"name\":\"monthly_receive\", \"kind\":\"monthly\", \"side\":\"receive\"},\n",
        "    {\"name\":\"monthly_send\",    \"kind\":\"monthly\", \"side\":\"send\"},\n",
        "    {\"name\":\"back_and_forth_transfers\", \"kind\":\"daily_pair_count\"},\n",
        "    ]\n",
        "    lazy_with_features = build_window_features_lazy(df, specs, amount_col=\"Amount\", label_choice=\"left\")\n",
        "    plan = (\n",
        "    lazy_with_features\n",
        "    .sort([\"Sender_account\", \"Date\"])\n",
        "    .with_columns([pl.col(\"Sender_account\").set_sorted(), pl.col(\"Date\").set_sorted()])\n",
        "    )\n",
        "    df_streamed = plan.collect(engine=\"streaming\")\n",
        "    df = df_streamed.sort(\"__row_idx\").drop(\"__row_idx\")\n",
        "\n",
        "    # More computation\n",
        "    lazy_with_derived = compute_derived_features_lazy(lazy_with_features)\n",
        "\n",
        "    # Before streaming collect: pick a primary grouping ordering that matches your rolling computations.\n",
        "    # If most rolling features used Receiver_account then Date, use that; otherwise use the grouping you chose.\n",
        "    plan_derived = (\n",
        "        lazy_with_derived\n",
        "        .sort([\"Sender_account\", \"Date\"])\n",
        "        .with_columns([pl.col(\"Sender_account\").set_sorted(), pl.col(\"Date\").set_sorted()])\n",
        "    )\n",
        "\n",
        "    df_streamed = plan_derived.collect(engine=\"streaming\")\n",
        "    df = df_streamed.sort(\"__row_idx\").drop(\"__row_idx\")\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "fMiWR6_xCPKx"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Recast**"
      ],
      "metadata": {
        "id": "CL3G1sSYDB5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "We recast the integer‑based columns following the logic rules outlined in the paper.\n",
        "\"Explainable Feature Engineering for Multi-class Money Laundering Classification\"\n",
        "This recasting is performed to optimize storage efficiency and reduce overall memory consumption.\"\n",
        "Excluding \"Sender_account\" and \"Receiver_account\" variables.\n",
        "\"\"\"\n",
        "\n",
        "def recast(df):\n",
        "    exclude = ['Sender_account', 'Receiver_account']\n",
        "\n",
        "    for col in df.columns:\n",
        "        if col not in exclude:\n",
        "            dtype = df[col].dtype\n",
        "            if dtype in (pl.Int64, pl.Int32):\n",
        "              maxval = df[col].max()\n",
        "              if maxval:\n",
        "                  if maxval < 127:\n",
        "                      df = df.with_columns(df[col].cast(pl.Int8).alias(col))\n",
        "                  elif maxval < 32767:\n",
        "                      df = df.with_columns(df[col].cast(pl.Int16).alias(col))\n",
        "                  elif maxval < 2147483647:\n",
        "                      df = df.with_columns(df[col].cast(pl.Int32).alias(col))\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "DUz_UzoQCRGv"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build graph network**"
      ],
      "metadata": {
        "id": "qbdO0Un0DGk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Compute circular_transaction_count using a\n",
        "calendar‑month sliding window, ensuring rustworkx is installed\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def circular_transaction_feature(df:pl.DataFrame):\n",
        "    # Iterate over monthly groups\n",
        "    results = []\n",
        "    for (year, month), group in df.group_by([\"year\", \"month\"]):\n",
        "        res = circular_count_monthly(group, year, month)\n",
        "        if res.height > 0:\n",
        "            results.append(res)\n",
        "\n",
        "    # Combine all results\n",
        "    out_rx = pl.concat(results, how=\"vertical\") if results else pl.DataFrame()\n",
        "\n",
        "    # Join back to original df\n",
        "    df_result = (\n",
        "        df.join(out_rx, on=[\"Sender_account\", \"year\", \"month\"], how=\"left\")\n",
        "        .with_columns(\n",
        "            pl.col(\"circular_transaction_count\").fill_null(0)\n",
        "        )\n",
        "    )\n",
        "    return df_result\n",
        "\n",
        "def circular_count_monthly(pdf, year, month):\n",
        "    edges = list(zip(pdf[\"Sender_account\"], pdf[\"Receiver_account\"]))\n",
        "    if not edges:\n",
        "        return empty_month_frame()\n",
        "\n",
        "    G = rx.PyDiGraph()\n",
        "    node_idx = {}\n",
        "    for u, v in edges:\n",
        "        if u not in node_idx:\n",
        "            node_idx[u] = G.add_node(u)\n",
        "        if v not in node_idx:\n",
        "            node_idx[v] = G.add_node(v)\n",
        "        G.add_edge(node_idx[u], node_idx[v], None)\n",
        "\n",
        "    cycles = rx.simple_cycles(G)\n",
        "\n",
        "    counter = {}\n",
        "    for cyc in cycles:\n",
        "        cyc_nodes = [G[node] for node in cyc]\n",
        "        for node in cyc_nodes:\n",
        "            counter[node] = counter.get(node, 0) + 1\n",
        "\n",
        "    return pl.DataFrame({\n",
        "        \"Sender_account\": list(counter.keys()),\n",
        "        \"circular_transaction_count\": list(counter.values()),\n",
        "        \"year\": [year] * len(counter),\n",
        "        \"month\": [month] * len(counter)\n",
        "    })"
      ],
      "metadata": {
        "id": "cLaxoS17CTvm"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import data**"
      ],
      "metadata": {
        "id": "KspPJYKyDLn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(\"synthetic_transaction_data\")\n",
        "df = pl.read_csv(\"synthetic_transaction_data/SAML-D.csv\")\n",
        "df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "ysRVaLXdCVqC",
        "outputId": "36dd7ae3-1032-4b08-80e0-7f28e5e337ef"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "shape: (5, 12)\n",
              "┌──────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
              "│ Time     ┆ Date      ┆ Sender_ac ┆ Receiver_ ┆ … ┆ Receiver_ ┆ Payment_t ┆ Is_launde ┆ Launderin │\n",
              "│ ---      ┆ ---       ┆ count     ┆ account   ┆   ┆ bank_loca ┆ ype       ┆ ring      ┆ g_type    │\n",
              "│ str      ┆ str       ┆ ---       ┆ ---       ┆   ┆ tion      ┆ ---       ┆ ---       ┆ ---       │\n",
              "│          ┆           ┆ i64       ┆ i64       ┆   ┆ ---       ┆ str       ┆ i64       ┆ str       │\n",
              "│          ┆           ┆           ┆           ┆   ┆ str       ┆           ┆           ┆           │\n",
              "╞══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
              "│ 10:35:19 ┆ 2022-10-0 ┆ 872473195 ┆ 276935542 ┆ … ┆ UK        ┆ Cash      ┆ 0         ┆ Normal_Ca │\n",
              "│          ┆ 7         ┆ 5         ┆ 6         ┆   ┆           ┆ Deposit   ┆           ┆ sh_Deposi │\n",
              "│          ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ ts        │\n",
              "│ 10:35:20 ┆ 2022-10-0 ┆ 149198906 ┆ 840125533 ┆ … ┆ UAE       ┆ Cross-bor ┆ 0         ┆ Normal_Fa │\n",
              "│          ┆ 7         ┆ 4         ┆ 5         ┆   ┆           ┆ der       ┆           ┆ n_Out     │\n",
              "│ 10:35:20 ┆ 2022-10-0 ┆ 287305149 ┆ 440476700 ┆ … ┆ UK        ┆ Cheque    ┆ 0         ┆ Normal_Sm │\n",
              "│          ┆ 7         ┆           ┆ 2         ┆   ┆           ┆           ┆           ┆ all_Fan_O │\n",
              "│          ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ ut        │\n",
              "│ 10:35:21 ┆ 2022-10-0 ┆ 537665243 ┆ 960042022 ┆ … ┆ UK        ┆ ACH       ┆ 0         ┆ Normal_Fa │\n",
              "│          ┆ 7         ┆ 7         ┆ 0         ┆   ┆           ┆           ┆           ┆ n_In      │\n",
              "│ 10:35:21 ┆ 2022-10-0 ┆ 961418617 ┆ 380333697 ┆ … ┆ UK        ┆ Cash      ┆ 0         ┆ Normal_Ca │\n",
              "│          ┆ 7         ┆ 8         ┆ 2         ┆   ┆           ┆ Deposit   ┆           ┆ sh_Deposi │\n",
              "│          ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ ts        │\n",
              "└──────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (5, 12)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Time</th><th>Date</th><th>Sender_account</th><th>Receiver_account</th><th>Amount</th><th>Payment_currency</th><th>Received_currency</th><th>Sender_bank_location</th><th>Receiver_bank_location</th><th>Payment_type</th><th>Is_laundering</th><th>Laundering_type</th></tr><tr><td>str</td><td>str</td><td>i64</td><td>i64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>str</td></tr></thead><tbody><tr><td>&quot;10:35:19&quot;</td><td>&quot;2022-10-07&quot;</td><td>8724731955</td><td>2769355426</td><td>1459.15</td><td>&quot;UK pounds&quot;</td><td>&quot;UK pounds&quot;</td><td>&quot;UK&quot;</td><td>&quot;UK&quot;</td><td>&quot;Cash Deposit&quot;</td><td>0</td><td>&quot;Normal_Cash_Deposits&quot;</td></tr><tr><td>&quot;10:35:20&quot;</td><td>&quot;2022-10-07&quot;</td><td>1491989064</td><td>8401255335</td><td>6019.64</td><td>&quot;UK pounds&quot;</td><td>&quot;Dirham&quot;</td><td>&quot;UK&quot;</td><td>&quot;UAE&quot;</td><td>&quot;Cross-border&quot;</td><td>0</td><td>&quot;Normal_Fan_Out&quot;</td></tr><tr><td>&quot;10:35:20&quot;</td><td>&quot;2022-10-07&quot;</td><td>287305149</td><td>4404767002</td><td>14328.44</td><td>&quot;UK pounds&quot;</td><td>&quot;UK pounds&quot;</td><td>&quot;UK&quot;</td><td>&quot;UK&quot;</td><td>&quot;Cheque&quot;</td><td>0</td><td>&quot;Normal_Small_Fan_Out&quot;</td></tr><tr><td>&quot;10:35:21&quot;</td><td>&quot;2022-10-07&quot;</td><td>5376652437</td><td>9600420220</td><td>11895.0</td><td>&quot;UK pounds&quot;</td><td>&quot;UK pounds&quot;</td><td>&quot;UK&quot;</td><td>&quot;UK&quot;</td><td>&quot;ACH&quot;</td><td>0</td><td>&quot;Normal_Fan_In&quot;</td></tr><tr><td>&quot;10:35:21&quot;</td><td>&quot;2022-10-07&quot;</td><td>9614186178</td><td>3803336972</td><td>115.25</td><td>&quot;UK pounds&quot;</td><td>&quot;UK pounds&quot;</td><td>&quot;UK&quot;</td><td>&quot;UK&quot;</td><td>&quot;Cash Deposit&quot;</td><td>0</td><td>&quot;Normal_Cash_Deposits&quot;</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convert Amount to log-Amount and convert strings to Datetime**"
      ],
      "metadata": {
        "id": "VusoxrivDPfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.with_columns(\n",
        "    pl.col(\"Amount\").log().alias(\"Amount\")\n",
        ")\n",
        "    # Convert datetime\n",
        "df = df.with_columns(\n",
        "    pl.col(\"Date\").str.strptime(pl.Date, \"%Y-%m-%d\").alias(\"Date\"),\n",
        "    pl.col(\"Time\").str.strptime(pl.Time, \"%H:%M:%S\").alias(\"Time\"))"
      ],
      "metadata": {
        "id": "JNstQP7yCYqi"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_val, df_test = custom_split_polars(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6GMHKTDCapr",
        "outputId": "fc88e4a2-8b85-4739-faa7-590dc5d15fc0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split complete:\n",
            "  Train: 6397772 rows (until 2023-05-10)\n",
            "  Val:   2076692 rows (2023-05-10–2023-07-19)\n",
            "  Test:  1030388 rows (after 2023-07-19)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = feature_engineering(df_train)\n",
        "df_train = circular_transaction_feature(df_train)\n",
        "df_train = recast(df_train)\n",
        "\n",
        "df_val = feature_engineering(df_val)\n",
        "df_val = circular_transaction_feature(df_val)\n",
        "df_val = recast(df_val)\n",
        "\n",
        "df_test = feature_engineering(df_test)\n",
        "df_test = circular_transaction_feature(df_test)\n",
        "df_test = recast(df_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BL6YqADZCcVT",
        "outputId": "71269e8e-a7ad-49d9-d98f-8289cad66396"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4033372652.py:249: UserWarning: Sortedness of columns cannot be checked when 'by' groups provided\n",
            "  df_streamed = plan.collect(engine=\"streaming\")\n",
            "/tmp/ipython-input-4033372652.py:171: PerformanceWarning: Resolving the schema of a LazyFrame is a potentially expensive operation. Use `LazyFrame.collect_schema()` to get the schema without this warning.\n",
            "  schema = lf.schema()\n",
            "/tmp/ipython-input-4033372652.py:263: UserWarning: Sortedness of columns cannot be checked when 'by' groups provided\n",
            "  df_streamed = plan_derived.collect(engine=\"streaming\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save dataframes**"
      ],
      "metadata": {
        "id": "t0sEPNL9DWFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Polars DataFrame as Parquet (efficient + preserves schema)\n",
        "df_train.write_parquet(os.path.join(drive_path, \"df_train.parquet\"))\n",
        "df_val.write_parquet(os.path.join(drive_path, \"df_val.parquet\"))\n",
        "df_test.write_parquet(os.path.join(drive_path, \"df_test.parquet\"))"
      ],
      "metadata": {
        "id": "JojpaxHfCe_Z"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}