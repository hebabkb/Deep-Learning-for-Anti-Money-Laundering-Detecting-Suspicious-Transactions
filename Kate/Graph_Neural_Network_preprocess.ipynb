{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7wlIpjIlL4P"
      },
      "source": [
        "# Graph Neural Network - Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgNVa-ywlS6b"
      },
      "source": [
        "**Import from Kaggle**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "jiOPsSfzifYq",
        "outputId": "84a81bd7-ac3b-443a-a334-739f55d04028"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2694e046-45b5-4825-8d22-290a5752a367\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2694e046-45b5-4825-8d22-290a5752a367\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"phuongkhanh21\",\"key\":\"0fd6eb70e1509aad441adeb76f0dfff2\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "drive_path = '/content/drive/MyDrive/Colab Notebooks/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvmKrb1TQhCT",
        "outputId": "bc1b3db7-2514-44e5-b2ce-2c930ea660d4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lj3RrHpBlaNt",
        "outputId": "338f4b83-e862-4f0d-9422-7df4462a2778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/berkanoztas/synthetic-transaction-monitoring-dataset-aml\n",
            "License(s): CC-BY-NC-SA-4.0\n",
            "Downloading synthetic-transaction-monitoring-dataset-aml.zip to /content\n",
            " 70% 135M/193M [00:00<00:00, 1.40GB/s]\n",
            "100% 193M/193M [00:00<00:00, 1.09GB/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "!mv kaggle.json /root/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle datasets download -d berkanoztas/synthetic-transaction-monitoring-dataset-aml\n",
        "\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"synthetic-transaction-monitoring-dataset-aml.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"synthetic_transaction_data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpzzwYeHlczC"
      },
      "source": [
        "**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MhrEJK-ogSA",
        "outputId": "e26859f9-9afb-4b1f-dd7b-ea022ca9a1f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rustworkx\n",
            "  Downloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from rustworkx) (2.0.2)\n",
            "Downloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/2.2 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rustworkx\n",
            "Successfully installed rustworkx-0.17.1\n"
          ]
        }
      ],
      "source": [
        "!pip install rustworkx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SX8RtxnelcXF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import polars as pl\n",
        "from datetime import timedelta\n",
        "import numpy as np\n",
        "import rustworkx as rx\n",
        "from typing import List, Dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXXmnDJkn1FF"
      },
      "source": [
        "**Featuring functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZbsyrBrtmSHq"
      },
      "outputs": [],
      "source": [
        "def custom_split_polars(df: pl.DataFrame, validation_dt: int = 70, test_dt: int = 35):\n",
        "    \"\"\"\n",
        "    Split a Polars DataFrame into train/validation/test by calendar-day cutoffs\n",
        "    measured backwards from the dataset max Date.\n",
        "\n",
        "    Parameters\n",
        "    - df: polars.DataFrame with a datetime column named \"Date\" (string or datetime OK)\n",
        "    - validation_dt: int days for validation window (e.g., 70)\n",
        "    - test_dt: int days for test window (e.g., 35)\n",
        "\n",
        "    Returns\n",
        "    - train_df, validation_df, test_df  (all eager polars.DataFrame)\n",
        "    \"\"\"\n",
        "\n",
        "    # ensure Date is a datetime type: try to get max, otherwise parse strings to Datetime\n",
        "    try:\n",
        "        max_date = df.select(pl.col(\"Date\").max()).to_series()[0]\n",
        "    except Exception:\n",
        "        df = df.with_column(pl.col(\"Date\").str.strptime(pl.Datetime, fmt=None).alias(\"Date\"))\n",
        "        max_date = df.select(pl.col(\"Date\").max()).to_series()[0]\n",
        "\n",
        "    test_cutoff = max_date - timedelta(days=test_dt)\n",
        "    validation_cutoff = max_date - timedelta(days=validation_dt)\n",
        "\n",
        "    test_set = df.filter(pl.col(\"Date\") >= pl.lit(test_cutoff))\n",
        "    validation_set = df.filter(\n",
        "        (pl.col(\"Date\") >= pl.lit(validation_cutoff)) & (pl.col(\"Date\") < pl.lit(test_cutoff))\n",
        "    )\n",
        "    train_set = df.filter(pl.col(\"Date\") < pl.lit(validation_cutoff))\n",
        "\n",
        "    return train_set, validation_set, test_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "x3psHfIKoHac"
      },
      "outputs": [],
      "source": [
        "def feature_engineering(df: pl.DataFrame) -> pl.DataFrame:\n",
        "    \"\"\"\n",
        "    Build and return engineered features from a Polars DataFrame.\n",
        "    \"\"\"\n",
        "\n",
        "    # Dropping column Laundering_type\n",
        "    df = df.drop(\"Laundering_type\")\n",
        "\n",
        "    # Temporal features\n",
        "    def temporal_features(df):\n",
        "\n",
        "        return df.with_columns([\n",
        "            df[\"Date\"].dt.year().alias(\"year\"),\n",
        "            df[\"Date\"].dt.month().alias(\"month\"),\n",
        "            df[\"Date\"].dt.day().alias(\"day_of_month\"),\n",
        "            df[\"Date\"].dt.weekday().alias(\"day_of_week\"),\n",
        "            df[\"Date\"].dt.ordinal_day().alias(\"day_of_year\"),\n",
        "            df[\"Time\"].dt.hour().alias(\"hour\"),\n",
        "            df[\"Time\"].dt.minute().alias(\"minute\"),\n",
        "            df[\"Time\"].dt.second().alias(\"second\"),\n",
        "        ])\n",
        "\n",
        "    # Risk features\n",
        "    high_risk_countries = ['Mexico', 'Turkey', 'Morocco', 'UAE']\n",
        "\n",
        "    def risk_features(df):\n",
        "\n",
        "        return df.with_columns([\n",
        "            (df[\"Payment_currency\"] != df[\"Received_currency\"]).cast(pl.Int8).alias(\"currency_mismatch\"),\n",
        "            (df[\"Payment_type\"] == \"Cross-border\").cast(pl.Int8).alias(\"cross_border\"),\n",
        "            df[\"Sender_bank_location\"].is_in(high_risk_countries).cast(pl.Int8).alias(\"high_risk_sender\"),\n",
        "            df[\"Receiver_bank_location\"].is_in(high_risk_countries).cast(pl.Int8).alias(\"high_risk_receiver\")])\n",
        "\n",
        "    def build_window_features_lazy(\n",
        "        df,\n",
        "        specs,\n",
        "        date_col=\"Date\",\n",
        "        sender_col=\"Sender_account\",\n",
        "        receiver_col=\"Receiver_account\",\n",
        "        amount_col=\"Amount\",\n",
        "        index_name=\"__row_idx\",\n",
        "        label_choice=\"left\",\n",
        "    ):\n",
        "        lf = df.lazy() if isinstance(df, pl.DataFrame) else df\n",
        "        lf = lf.with_columns(pl.arange(0, pl.len()).over(pl.lit(True)).alias(index_name))\n",
        "        out_lf = lf\n",
        "\n",
        "        for spec in specs:\n",
        "            kind = spec.get(\"kind\", \"rolling\")\n",
        "\n",
        "            if kind == \"rolling\":\n",
        "                # existing rolling logic (no change)\n",
        "                name = spec[\"name\"]\n",
        "                direction = spec[\"type\"]  # \"fanin\" or \"fanout\"\n",
        "                period_days = int(spec[\"period_days\"])\n",
        "                every = spec.get(\"every\", \"1d\")\n",
        "\n",
        "                if direction == \"fanin\":\n",
        "                    group_by = receiver_col\n",
        "                    agg_on = sender_col\n",
        "                else:\n",
        "                    group_by = sender_col\n",
        "                    agg_on = receiver_col\n",
        "\n",
        "                win_label = label_choice\n",
        "                strategy = \"forward\" if win_label == \"left\" else \"backward\"\n",
        "\n",
        "                right = (\n",
        "                    lf\n",
        "                    .sort([group_by, date_col])\n",
        "                    .group_by_dynamic(\n",
        "                        index_column=date_col,\n",
        "                        every=every,\n",
        "                        period=f\"{period_days}d\",\n",
        "                        group_by=group_by,\n",
        "                        closed=\"both\",\n",
        "                        label=win_label\n",
        "                    )\n",
        "                    .agg(pl.col(agg_on).n_unique().alias(name))\n",
        "                    .sort([group_by, date_col])\n",
        "                )\n",
        "\n",
        "                left = out_lf.sort([group_by, date_col])\n",
        "\n",
        "                out_lf = left.join_asof(\n",
        "                    right,\n",
        "                    left_on=date_col,\n",
        "                    right_on=date_col,\n",
        "                    by=group_by,\n",
        "                    strategy=strategy,\n",
        "                )\n",
        "\n",
        "            elif kind == \"monthly\":\n",
        "                # existing monthly logic (no change)\n",
        "                name = spec[\"name\"]\n",
        "                side = spec.get(\"side\", \"receive\")\n",
        "                group_col = receiver_col if side == \"receive\" else sender_col\n",
        "\n",
        "                monthly_agg = (\n",
        "                    lf\n",
        "                    .with_columns(pl.col(date_col).dt.truncate(\"1mo\").alias(\"__month\"))\n",
        "                    .group_by([group_col, \"__month\"])\n",
        "                    .agg(pl.col(amount_col).sum().alias(name))\n",
        "                )\n",
        "\n",
        "                out_lf = (\n",
        "                    out_lf\n",
        "                    .with_columns(pl.col(date_col).dt.truncate(\"1mo\").alias(\"__month\"))\n",
        "                    .join(monthly_agg, on=[group_col, \"__month\"], how=\"left\")\n",
        "                    .drop(\"__month\")\n",
        "                )\n",
        "\n",
        "            elif kind == \"daily_pair_count\":\n",
        "                # NEW: back_and_forth_transfers (exact-match on day + pair)\n",
        "                name = spec[\"name\"]  # e.g., \"back_and_forth_transfers\"\n",
        "                # day key = calendar day (truncate to 1 day)\n",
        "                day_key = \"__day\"\n",
        "                # compute counts per sender/receiver/day using lf (lazy)\n",
        "                pair_daily_agg = (\n",
        "                    lf\n",
        "                    .with_columns(pl.col(date_col).dt.truncate(\"1d\").alias(day_key))\n",
        "                    .group_by([sender_col, receiver_col, day_key])\n",
        "                    .agg(pl.len().alias(name))  # .len() counts rows in group\n",
        "                )\n",
        "\n",
        "                # attach day key to working frame and join exact on pair + day\n",
        "                out_lf = (\n",
        "                    out_lf\n",
        "                    .with_columns(pl.col(date_col).dt.truncate(\"1d\").alias(day_key))\n",
        "                    .join(pair_daily_agg, on=[sender_col, receiver_col, day_key], how=\"left\")\n",
        "                    .fill_null(0)       # optional: replace nulls with 0\n",
        "                    .with_columns(pl.col(name).cast(pl.Int64))  # ensure integer type\n",
        "                    .drop(day_key)\n",
        "                )\n",
        "\n",
        "            else:\n",
        "                raise ValueError(\"spec kind must be 'rolling', 'monthly', or 'daily_pair_count'\")\n",
        "\n",
        "        return out_lf\n",
        "\n",
        "\n",
        "    def compute_derived_features_lazy(\n",
        "        lf: pl.LazyFrame,\n",
        "        *,\n",
        "        fanin_col: str = \"fanin_30d\",\n",
        "        fanout_col: str = \"fanout_30d\",\n",
        "        daily_receive_col: str = \"daily_receive\",\n",
        "        monthly_receive_col: str = \"monthly_receive\",\n",
        "        monthly_send_col: str = \"monthly_send\",\n",
        "        amount_col: str = \"Amount\",\n",
        "        sender_col: str = \"Sender_account\",\n",
        "        receiver_col: str = \"Receiver_account\",\n",
        "        index_name: str = \"__row_idx\",\n",
        "    ) -> pl.LazyFrame:\n",
        "        \"\"\"\n",
        "        Take a LazyFrame and return a LazyFrame with derived features:\n",
        "          - fan_in_out_ratio (safe division, 0 when denom missing or zero)\n",
        "          - fanin_intensity_ratio (fanin_30d / daily_receive, 0 when denom missing or zero)\n",
        "          - amount_dispersion_std (per-sender std of Amount, filled 0 when null)\n",
        "          - sent_to_received_ratio_monthly (monthly_receive / monthly_send, 0 when denom missing or zero)\n",
        "\n",
        "        If `daily_receive` is not present in lf.schema(), it is computed lazily as the\n",
        "        per-receiver unique-senders per calendar day (dt.truncate(\"1d\")) and joined back.\n",
        "        The function is fully lazy; call .collect(...) when ready.\n",
        "        \"\"\"\n",
        "        # ensure lazy input\n",
        "        lf = lf if isinstance(lf, pl.LazyFrame) else lf.lazy()\n",
        "\n",
        "        # Attempt to read schema; if unavailable assume missing and compute\n",
        "        try:\n",
        "            schema = lf.schema()\n",
        "            has_daily = daily_receive_col in schema\n",
        "        except Exception:\n",
        "            has_daily = False\n",
        "\n",
        "        # If daily_receive missing, compute it lazily (exact day bucket of unique senders per receiver)\n",
        "        if not has_daily:\n",
        "            day_key = \"__day_for_daily_receive\"\n",
        "            daily_receive_agg = (\n",
        "                lf\n",
        "                .with_columns(pl.col(\"Date\").dt.truncate(\"1d\").alias(day_key))\n",
        "                .group_by([receiver_col, day_key])\n",
        "                .agg(pl.col(sender_col).n_unique().alias(daily_receive_col))\n",
        "            )\n",
        "            lf = (\n",
        "                lf\n",
        "                .with_columns(pl.col(\"Date\").dt.truncate(\"1d\").alias(day_key))\n",
        "                .join(daily_receive_agg, on=[receiver_col, day_key], how=\"left\")\n",
        "                .drop(day_key)\n",
        "            )\n",
        "\n",
        "        # safe division helper expression\n",
        "        def safe_div_expr(num: str, den: str, out_name: str):\n",
        "            return (\n",
        "                pl.when(pl.col(den).is_null() | (pl.col(den) == 0))\n",
        "                  .then(0.0)\n",
        "                  .otherwise(pl.col(num).cast(pl.Float64) / pl.col(den).cast(pl.Float64))\n",
        "                  .alias(out_name)\n",
        "            )\n",
        "\n",
        "        fan_in_out_expr = safe_div_expr(fanin_col, fanout_col, \"fan_in_out_ratio\")\n",
        "        fanin_intensity_expr = safe_div_expr(fanin_col, daily_receive_col, \"fanin_intensity_ratio\")\n",
        "        sent_to_received_monthly_expr = safe_div_expr(monthly_receive_col, monthly_send_col, \"sent_to_received_ratio_monthly\")\n",
        "\n",
        "        # per-sender std aggregation (lazy) and join back\n",
        "        sender_std_agg = (\n",
        "            lf\n",
        "            .select([sender_col, amount_col])\n",
        "            .group_by(sender_col)\n",
        "            .agg(pl.col(amount_col).std().alias(\"__amount_std\"))\n",
        "        )\n",
        "\n",
        "        out = (\n",
        "            lf\n",
        "            .join(sender_std_agg, on=sender_col, how=\"left\")\n",
        "            .with_columns(\n",
        "                pl.col(\"__amount_std\").cast(pl.Float64).fill_null(0.0).alias(\"amount_dispersion_std\")\n",
        "            )\n",
        "            .drop(\"__amount_std\")\n",
        "            .with_columns([\n",
        "                fan_in_out_expr,\n",
        "                fanin_intensity_expr,\n",
        "                sent_to_received_monthly_expr\n",
        "            ])\n",
        "        )\n",
        "\n",
        "        return out\n",
        "    # Temporal features\n",
        "    df = temporal_features(df)\n",
        "\n",
        "    # Risk features\n",
        "    df = risk_features(df)\n",
        "\n",
        "    # Rolling window computing\n",
        "    specs = [\n",
        "    {\"name\":\"fanin_30d\", \"kind\":\"rolling\", \"type\":\"fanin\", \"period_days\":30, \"every\":\"1d\"},\n",
        "    {\"name\":\"fanout_30d\", \"kind\":\"rolling\", \"type\":\"fanout\", \"period_days\":30, \"every\":\"1d\"},\n",
        "    {\"name\":\"daily_recieve\", \"kind\":\"rolling\", \"type\":\"fanin\", \"period_days\":1, \"every\":\"1d\"},\n",
        "    {\"name\":\"monthly_receive\", \"kind\":\"monthly\", \"side\":\"receive\"},\n",
        "    {\"name\":\"monthly_send\",    \"kind\":\"monthly\", \"side\":\"send\"},\n",
        "    {\"name\":\"back_and_forth_transfers\", \"kind\":\"daily_pair_count\"},\n",
        "    ]\n",
        "    lazy_with_features = build_window_features_lazy(df, specs, amount_col=\"Amount\", label_choice=\"left\")\n",
        "    plan = (\n",
        "    lazy_with_features\n",
        "    .sort([\"Sender_account\", \"Date\"])\n",
        "    .with_columns([pl.col(\"Sender_account\").set_sorted(), pl.col(\"Date\").set_sorted()])\n",
        "    )\n",
        "    df_streamed = plan.collect(engine=\"streaming\")\n",
        "    df = df_streamed.sort(\"__row_idx\").drop(\"__row_idx\")\n",
        "\n",
        "    # More computation\n",
        "    lazy_with_derived = compute_derived_features_lazy(lazy_with_features)\n",
        "\n",
        "    # Before streaming collect: pick a primary grouping ordering that matches your rolling computations.\n",
        "    # If most rolling features used Receiver_account then Date, use that; otherwise use the grouping you chose.\n",
        "    plan_derived = (\n",
        "        lazy_with_derived\n",
        "        .sort([\"Sender_account\", \"Date\"])\n",
        "        .with_columns([pl.col(\"Sender_account\").set_sorted(), pl.col(\"Date\").set_sorted()])\n",
        "    )\n",
        "\n",
        "    df_streamed = plan_derived.collect(engine=\"streaming\")\n",
        "    df = df_streamed.sort(\"__row_idx\").drop(\"__row_idx\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MSc8m8CnoMT-"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "We recast the integer‑based columns following the logic rules outlined in the paper.\n",
        "\"Explainable Feature Engineering for Multi-class Money Laundering Classification\"\n",
        "This recasting is performed to optimize storage efficiency and reduce overall memory consumption.\"\n",
        "Excluding \"Sender_account\" and \"Receiver_account\" variables.\n",
        "\"\"\"\n",
        "\n",
        "def recast(df):\n",
        "    exclude = ['Sender_account', 'Receiver_account']\n",
        "\n",
        "    for col in df.columns:\n",
        "        if col not in exclude:\n",
        "            dtype = df[col].dtype\n",
        "            if dtype in (pl.Int64, pl.Int32):\n",
        "              maxval = df[col].max()\n",
        "              if maxval:\n",
        "                  if maxval < 127:\n",
        "                      df = df.with_columns(df[col].cast(pl.Int8).alias(col))\n",
        "                  elif maxval < 32767:\n",
        "                      df = df.with_columns(df[col].cast(pl.Int16).alias(col))\n",
        "                  elif maxval < 2147483647:\n",
        "                      df = df.with_columns(df[col].cast(pl.Int32).alias(col))\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2lYdmOAtoQrU"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Compute circular_transaction_count using a\n",
        "calendar‑month sliding window, ensuring rustworkx is installed\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def circular_transaction_feature(df:pl.DataFrame):\n",
        "    # Iterate over monthly groups\n",
        "    results = []\n",
        "    for (year, month), group in df.group_by([\"year\", \"month\"]):\n",
        "        res = circular_count_monthly(group, year, month)\n",
        "        if res.height > 0:\n",
        "            results.append(res)\n",
        "\n",
        "    # Combine all results\n",
        "    out_rx = pl.concat(results, how=\"vertical\") if results else pl.DataFrame()\n",
        "\n",
        "    # Join back to original df\n",
        "    df_result = (\n",
        "        df.join(out_rx, on=[\"Sender_account\", \"year\", \"month\"], how=\"left\")\n",
        "        .with_columns(\n",
        "            pl.col(\"circular_transaction_count\").fill_null(0)\n",
        "        )\n",
        "    )\n",
        "    return df_result\n",
        "\n",
        "def circular_count_monthly(pdf, year, month):\n",
        "    edges = list(zip(pdf[\"Sender_account\"], pdf[\"Receiver_account\"]))\n",
        "    if not edges:\n",
        "        return empty_month_frame()\n",
        "\n",
        "    G = rx.PyDiGraph()\n",
        "    node_idx = {}\n",
        "    for u, v in edges:\n",
        "        if u not in node_idx:\n",
        "            node_idx[u] = G.add_node(u)\n",
        "        if v not in node_idx:\n",
        "            node_idx[v] = G.add_node(v)\n",
        "        G.add_edge(node_idx[u], node_idx[v], None)\n",
        "\n",
        "    cycles = rx.simple_cycles(G)\n",
        "\n",
        "    counter = {}\n",
        "    for cyc in cycles:\n",
        "        cyc_nodes = [G[node] for node in cyc]\n",
        "        for node in cyc_nodes:\n",
        "            counter[node] = counter.get(node, 0) + 1\n",
        "\n",
        "    return pl.DataFrame({\n",
        "        \"Sender_account\": list(counter.keys()),\n",
        "        \"circular_transaction_count\": list(counter.values()),\n",
        "        \"year\": [year] * len(counter),\n",
        "        \"month\": [month] * len(counter)\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfvtRwGYot97"
      },
      "source": [
        "**Prepare data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "PfWDu8k0owTj",
        "outputId": "8403a99a-a4b1-41ff-cab5-503155d230b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "shape: (5, 12)\n",
              "┌──────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
              "│ Time     ┆ Date      ┆ Sender_ac ┆ Receiver_ ┆ … ┆ Receiver_ ┆ Payment_t ┆ Is_launde ┆ Launderin │\n",
              "│ ---      ┆ ---       ┆ count     ┆ account   ┆   ┆ bank_loca ┆ ype       ┆ ring      ┆ g_type    │\n",
              "│ str      ┆ str       ┆ ---       ┆ ---       ┆   ┆ tion      ┆ ---       ┆ ---       ┆ ---       │\n",
              "│          ┆           ┆ i64       ┆ i64       ┆   ┆ ---       ┆ str       ┆ i64       ┆ str       │\n",
              "│          ┆           ┆           ┆           ┆   ┆ str       ┆           ┆           ┆           │\n",
              "╞══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
              "│ 10:35:19 ┆ 2022-10-0 ┆ 872473195 ┆ 276935542 ┆ … ┆ UK        ┆ Cash      ┆ 0         ┆ Normal_Ca │\n",
              "│          ┆ 7         ┆ 5         ┆ 6         ┆   ┆           ┆ Deposit   ┆           ┆ sh_Deposi │\n",
              "│          ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ ts        │\n",
              "│ 10:35:20 ┆ 2022-10-0 ┆ 149198906 ┆ 840125533 ┆ … ┆ UAE       ┆ Cross-bor ┆ 0         ┆ Normal_Fa │\n",
              "│          ┆ 7         ┆ 4         ┆ 5         ┆   ┆           ┆ der       ┆           ┆ n_Out     │\n",
              "│ 10:35:20 ┆ 2022-10-0 ┆ 287305149 ┆ 440476700 ┆ … ┆ UK        ┆ Cheque    ┆ 0         ┆ Normal_Sm │\n",
              "│          ┆ 7         ┆           ┆ 2         ┆   ┆           ┆           ┆           ┆ all_Fan_O │\n",
              "│          ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ ut        │\n",
              "│ 10:35:21 ┆ 2022-10-0 ┆ 537665243 ┆ 960042022 ┆ … ┆ UK        ┆ ACH       ┆ 0         ┆ Normal_Fa │\n",
              "│          ┆ 7         ┆ 7         ┆ 0         ┆   ┆           ┆           ┆           ┆ n_In      │\n",
              "│ 10:35:21 ┆ 2022-10-0 ┆ 961418617 ┆ 380333697 ┆ … ┆ UK        ┆ Cash      ┆ 0         ┆ Normal_Ca │\n",
              "│          ┆ 7         ┆ 8         ┆ 2         ┆   ┆           ┆ Deposit   ┆           ┆ sh_Deposi │\n",
              "│          ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ ts        │\n",
              "└──────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (5, 12)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Time</th><th>Date</th><th>Sender_account</th><th>Receiver_account</th><th>Amount</th><th>Payment_currency</th><th>Received_currency</th><th>Sender_bank_location</th><th>Receiver_bank_location</th><th>Payment_type</th><th>Is_laundering</th><th>Laundering_type</th></tr><tr><td>str</td><td>str</td><td>i64</td><td>i64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>str</td></tr></thead><tbody><tr><td>&quot;10:35:19&quot;</td><td>&quot;2022-10-07&quot;</td><td>8724731955</td><td>2769355426</td><td>1459.15</td><td>&quot;UK pounds&quot;</td><td>&quot;UK pounds&quot;</td><td>&quot;UK&quot;</td><td>&quot;UK&quot;</td><td>&quot;Cash Deposit&quot;</td><td>0</td><td>&quot;Normal_Cash_Deposits&quot;</td></tr><tr><td>&quot;10:35:20&quot;</td><td>&quot;2022-10-07&quot;</td><td>1491989064</td><td>8401255335</td><td>6019.64</td><td>&quot;UK pounds&quot;</td><td>&quot;Dirham&quot;</td><td>&quot;UK&quot;</td><td>&quot;UAE&quot;</td><td>&quot;Cross-border&quot;</td><td>0</td><td>&quot;Normal_Fan_Out&quot;</td></tr><tr><td>&quot;10:35:20&quot;</td><td>&quot;2022-10-07&quot;</td><td>287305149</td><td>4404767002</td><td>14328.44</td><td>&quot;UK pounds&quot;</td><td>&quot;UK pounds&quot;</td><td>&quot;UK&quot;</td><td>&quot;UK&quot;</td><td>&quot;Cheque&quot;</td><td>0</td><td>&quot;Normal_Small_Fan_Out&quot;</td></tr><tr><td>&quot;10:35:21&quot;</td><td>&quot;2022-10-07&quot;</td><td>5376652437</td><td>9600420220</td><td>11895.0</td><td>&quot;UK pounds&quot;</td><td>&quot;UK pounds&quot;</td><td>&quot;UK&quot;</td><td>&quot;UK&quot;</td><td>&quot;ACH&quot;</td><td>0</td><td>&quot;Normal_Fan_In&quot;</td></tr><tr><td>&quot;10:35:21&quot;</td><td>&quot;2022-10-07&quot;</td><td>9614186178</td><td>3803336972</td><td>115.25</td><td>&quot;UK pounds&quot;</td><td>&quot;UK pounds&quot;</td><td>&quot;UK&quot;</td><td>&quot;UK&quot;</td><td>&quot;Cash Deposit&quot;</td><td>0</td><td>&quot;Normal_Cash_Deposits&quot;</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "os.listdir(\"synthetic_transaction_data\")\n",
        "df = pl.read_csv(\"synthetic_transaction_data/SAML-D.csv\")\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jjVBDisrolpp"
      },
      "outputs": [],
      "source": [
        "df = df.with_columns(\n",
        "    pl.col(\"Amount\").log().alias(\"Amount\")\n",
        ")\n",
        "    # Convert datetime\n",
        "df = df.with_columns(\n",
        "    pl.col(\"Date\").str.strptime(pl.Date, \"%Y-%m-%d\").alias(\"Date\"),\n",
        "    pl.col(\"Time\").str.strptime(pl.Time, \"%H:%M:%S\").alias(\"Time\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "KIAML5G0oqDz"
      },
      "outputs": [],
      "source": [
        "df_train, df_val, df_test = custom_split_polars(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "JgQNNcjxpCm5",
        "outputId": "164f5445-2b89-40a5-9dca-1b50c4c04906"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "shape: (5, 12)\n",
              "┌──────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
              "│ Time     ┆ Date      ┆ Sender_ac ┆ Receiver_ ┆ … ┆ Receiver_ ┆ Payment_t ┆ Is_launde ┆ Launderin │\n",
              "│ ---      ┆ ---       ┆ count     ┆ account   ┆   ┆ bank_loca ┆ ype       ┆ ring      ┆ g_type    │\n",
              "│ time     ┆ date      ┆ ---       ┆ ---       ┆   ┆ tion      ┆ ---       ┆ ---       ┆ ---       │\n",
              "│          ┆           ┆ i64       ┆ i64       ┆   ┆ ---       ┆ str       ┆ i64       ┆ str       │\n",
              "│          ┆           ┆           ┆           ┆   ┆ str       ┆           ┆           ┆           │\n",
              "╞══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
              "│ 10:35:19 ┆ 2022-10-0 ┆ 872473195 ┆ 276935542 ┆ … ┆ UK        ┆ Cash      ┆ 0         ┆ Normal_Ca │\n",
              "│          ┆ 7         ┆ 5         ┆ 6         ┆   ┆           ┆ Deposit   ┆           ┆ sh_Deposi │\n",
              "│          ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ ts        │\n",
              "│ 10:35:20 ┆ 2022-10-0 ┆ 149198906 ┆ 840125533 ┆ … ┆ UAE       ┆ Cross-bor ┆ 0         ┆ Normal_Fa │\n",
              "│          ┆ 7         ┆ 4         ┆ 5         ┆   ┆           ┆ der       ┆           ┆ n_Out     │\n",
              "│ 10:35:20 ┆ 2022-10-0 ┆ 287305149 ┆ 440476700 ┆ … ┆ UK        ┆ Cheque    ┆ 0         ┆ Normal_Sm │\n",
              "│          ┆ 7         ┆           ┆ 2         ┆   ┆           ┆           ┆           ┆ all_Fan_O │\n",
              "│          ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ ut        │\n",
              "│ 10:35:21 ┆ 2022-10-0 ┆ 537665243 ┆ 960042022 ┆ … ┆ UK        ┆ ACH       ┆ 0         ┆ Normal_Fa │\n",
              "│          ┆ 7         ┆ 7         ┆ 0         ┆   ┆           ┆           ┆           ┆ n_In      │\n",
              "│ 10:35:21 ┆ 2022-10-0 ┆ 961418617 ┆ 380333697 ┆ … ┆ UK        ┆ Cash      ┆ 0         ┆ Normal_Ca │\n",
              "│          ┆ 7         ┆ 8         ┆ 2         ┆   ┆           ┆ Deposit   ┆           ┆ sh_Deposi │\n",
              "│          ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ ts        │\n",
              "└──────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (5, 12)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Time</th><th>Date</th><th>Sender_account</th><th>Receiver_account</th><th>Amount</th><th>Payment_currency</th><th>Received_currency</th><th>Sender_bank_location</th><th>Receiver_bank_location</th><th>Payment_type</th><th>Is_laundering</th><th>Laundering_type</th></tr><tr><td>time</td><td>date</td><td>i64</td><td>i64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>str</td></tr></thead><tbody><tr><td>10:35:19</td><td>2022-10-07</td><td>8724731955</td><td>2769355426</td><td>7.285609</td><td>&quot;UK pounds&quot;</td><td>&quot;UK pounds&quot;</td><td>&quot;UK&quot;</td><td>&quot;UK&quot;</td><td>&quot;Cash Deposit&quot;</td><td>0</td><td>&quot;Normal_Cash_Deposits&quot;</td></tr><tr><td>10:35:20</td><td>2022-10-07</td><td>1491989064</td><td>8401255335</td><td>8.702783</td><td>&quot;UK pounds&quot;</td><td>&quot;Dirham&quot;</td><td>&quot;UK&quot;</td><td>&quot;UAE&quot;</td><td>&quot;Cross-border&quot;</td><td>0</td><td>&quot;Normal_Fan_Out&quot;</td></tr><tr><td>10:35:20</td><td>2022-10-07</td><td>287305149</td><td>4404767002</td><td>9.570002</td><td>&quot;UK pounds&quot;</td><td>&quot;UK pounds&quot;</td><td>&quot;UK&quot;</td><td>&quot;UK&quot;</td><td>&quot;Cheque&quot;</td><td>0</td><td>&quot;Normal_Small_Fan_Out&quot;</td></tr><tr><td>10:35:21</td><td>2022-10-07</td><td>5376652437</td><td>9600420220</td><td>9.383873</td><td>&quot;UK pounds&quot;</td><td>&quot;UK pounds&quot;</td><td>&quot;UK&quot;</td><td>&quot;UK&quot;</td><td>&quot;ACH&quot;</td><td>0</td><td>&quot;Normal_Fan_In&quot;</td></tr><tr><td>10:35:21</td><td>2022-10-07</td><td>9614186178</td><td>3803336972</td><td>4.747104</td><td>&quot;UK pounds&quot;</td><td>&quot;UK pounds&quot;</td><td>&quot;UK&quot;</td><td>&quot;UK&quot;</td><td>&quot;Cash Deposit&quot;</td><td>0</td><td>&quot;Normal_Cash_Deposits&quot;</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvrTjXsbphtW",
        "outputId": "77b7a725-f048-4d57-d2e9-1debb3c756ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4033372652.py:249: UserWarning: Sortedness of columns cannot be checked when 'by' groups provided\n",
            "  df_streamed = plan.collect(engine=\"streaming\")\n",
            "/tmp/ipython-input-4033372652.py:171: PerformanceWarning: Resolving the schema of a LazyFrame is a potentially expensive operation. Use `LazyFrame.collect_schema()` to get the schema without this warning.\n",
            "  schema = lf.schema()\n",
            "/tmp/ipython-input-4033372652.py:263: UserWarning: Sortedness of columns cannot be checked when 'by' groups provided\n",
            "  df_streamed = plan_derived.collect(engine=\"streaming\")\n"
          ]
        }
      ],
      "source": [
        "df_train = feature_engineering(df_train)\n",
        "df_train = circular_transaction_feature(df_train)\n",
        "df_train = recast(df_train)\n",
        "\n",
        "df_val = feature_engineering(df_val)\n",
        "df_val = circular_transaction_feature(df_val)\n",
        "df_val = recast(df_val)\n",
        "\n",
        "df_test = feature_engineering(df_test)\n",
        "df_test = circular_transaction_feature(df_test)\n",
        "df_test = recast(df_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJTXRUkry7hl"
      },
      "source": [
        "**GCN preparation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "O6l4uJL9Ayim"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9X-Nr8m2Gzi"
      },
      "source": [
        "**Encoding accounts**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "IDiPW-3Xy9-t"
      },
      "outputs": [],
      "source": [
        "# Mapping from train only\n",
        "accounts = pl.concat([df_train[\"Sender_account\"], df_train[\"Receiver_account\"]]).unique()\n",
        "mapping_df = pl.DataFrame({\"account\": accounts, \"id\": list(range(len(accounts)))})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "bWRos1O02MtS"
      },
      "outputs": [],
      "source": [
        "def map_accounts(df):\n",
        "    # join for sender\n",
        "    df = df.join(mapping_df, left_on=\"Sender_account\", right_on=\"account\", how=\"left\") \\\n",
        "                   .with_columns(pl.col(\"id\").alias(\"src\")) \\\n",
        "                   .drop(\"id\")\n",
        "    # join for receiver\n",
        "    df = df.join(mapping_df, left_on=\"Receiver_account\", right_on=\"account\", how=\"left\") \\\n",
        "                   .with_columns(pl.col(\"id\").alias(\"dst\")) \\\n",
        "                   .drop(\"id\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "3kQkW9BG2SsX"
      },
      "outputs": [],
      "source": [
        "df_train = map_accounts(df_train)\n",
        "df_val = map_accounts(df_val)\n",
        "df_test = map_accounts(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "tbXKjW-IBtBo"
      },
      "outputs": [],
      "source": [
        "# Then fill_null with -1\n",
        "df_train = df_train.with_columns([\n",
        "    pl.col(\"src\").fill_null(-1).cast(pl.Int64),\n",
        "    pl.col(\"dst\").fill_null(-1).cast(pl.Int64),\n",
        "])\n",
        "df_val = df_val.with_columns([\n",
        "    pl.col(\"src\").fill_null(-1).cast(pl.Int64),\n",
        "    pl.col(\"dst\").fill_null(-1).cast(pl.Int64),\n",
        "])\n",
        "df_test = df_test.with_columns([\n",
        "    pl.col(\"src\").fill_null(-1).cast(pl.Int64),\n",
        "    pl.col(\"dst\").fill_null(-1).cast(pl.Int64),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXK4isCt4qpE"
      },
      "source": [
        "**Build edge features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "PIArlTFi4SCd"
      },
      "outputs": [],
      "source": [
        "edge_numeric = [\n",
        "    \"Amount\",\n",
        "    \"fanin_30d\",\n",
        "    \"fanin_intensity_ratio\",\n",
        "    \"amount_dispersion_std\",\n",
        "    \"sent_to_received_ratio_monthly\",\n",
        "    \"back_and_forth_transfers\",\n",
        "    \"circular_transaction_count\"\n",
        "]\n",
        "edge_flags = [\"currency_mismatch\", \"high_risk_sender\", \"high_risk_receiver\"]\n",
        "temporal_cols = [\"year\", \"day_of_month\", \"day_of_week\", \"hour\", \"minute\", \"second\"]\n",
        "label_col = \"Is_laundering\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_J5Smm1z42di"
      },
      "outputs": [],
      "source": [
        "def build_edge_attr_from_polars(\n",
        "    df: pl.DataFrame,\n",
        "    edge_numeric: list,\n",
        "    edge_flags: list,\n",
        "    temporal_cols: list,\n",
        "    edge_num_mean: np.ndarray = None,\n",
        "    edge_num_std: np.ndarray = None,\n",
        "    log_amount_col: str = \"Amount\",\n",
        "    return_scalers: bool = False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Build edge feature tensor (edge_attr) for a Polars DataFrame of transactions.\n",
        "    Designed to be identical across train/val/test splits.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pl.DataFrame\n",
        "        Must contain columns: src, dst, edge_numeric, edge_flags, temporal_cols.\n",
        "    edge_numeric : list[str]\n",
        "        Names of numeric transaction-level columns (e.g., [\"Amount\", \"fanin_30d\", ...])\n",
        "    edge_flags : list[str]\n",
        "        Names of binary/categorical indicator columns (0/1 flags).\n",
        "    temporal_cols : list[str]\n",
        "        Temporal features to encode cyclically: [\"year\", \"day_of_month\", \"day_of_week\", \"hour\", \"minute\", \"second\"]\n",
        "    edge_num_mean, edge_num_std : np.ndarray or None\n",
        "        Normalization statistics from the training set. If None, they will be computed.\n",
        "    log_amount_col : str\n",
        "        Name of numeric column to log-transform (e.g., \"Amount\"). If not in edge_numeric, no effect.\n",
        "    return_scalers : bool\n",
        "        If True, returns (edge_attr, mean, std). Otherwise just edge_attr.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    edge_attr : torch.FloatTensor [E, F_edge]\n",
        "    edge_num_mean, edge_num_std : np.ndarray\n",
        "        Returned only if return_scalers=True\n",
        "    \"\"\"\n",
        "\n",
        "    # Cast to correct types\n",
        "    df = df.with_columns(\n",
        "        [pl.col(c).cast(pl.Float64) for c in edge_numeric] +\n",
        "        [pl.col(c).cast(pl.Int32) for c in edge_flags] +\n",
        "        [pl.col(c).cast(pl.Int32) for c in temporal_cols]\n",
        "    )\n",
        "\n",
        "    # Extract numeric + flag columns\n",
        "    num_flags_np = df.select(edge_numeric + edge_flags).to_numpy().astype(np.float32)\n",
        "    num_count = len(edge_numeric)\n",
        "    flag_count = len(edge_flags)\n",
        "    num_np = num_flags_np[:, :num_count]\n",
        "    flags_np = num_flags_np[:, num_count:]\n",
        "\n",
        "    # Optional log transform for amount\n",
        "    if log_amount_col in edge_numeric:\n",
        "        amt_idx = edge_numeric.index(log_amount_col)\n",
        "        num_np[:, amt_idx] = np.log1p(np.clip(num_np[:, amt_idx], a_min=0, a_max=None))\n",
        "\n",
        "    # Compute normalization scalers if needed\n",
        "    if edge_num_mean is None or edge_num_std is None:\n",
        "        edge_num_mean = num_np.mean(axis=0)\n",
        "        edge_num_std = num_np.std(axis=0) + 1e-9\n",
        "\n",
        "    # Apply normalization\n",
        "    num_np_norm = (num_np - edge_num_mean[None, :]) / edge_num_std[None, :]\n",
        "\n",
        "    # Temporal cyclic encodings\n",
        "    tmp = df.select(temporal_cols).to_numpy().astype(np.float32)\n",
        "    years = tmp[:, 0]\n",
        "    dom   = tmp[:, 1]\n",
        "    dow   = tmp[:, 2]\n",
        "    hour  = tmp[:, 3]\n",
        "    minute= tmp[:, 4]\n",
        "    second= tmp[:, 5]\n",
        "\n",
        "    def cyc_np(x, period):\n",
        "        ang = 2 * np.pi * x / period\n",
        "        return np.sin(ang).astype(np.float32), np.cos(ang).astype(np.float32)\n",
        "\n",
        "    dom_sin, dom_cos   = cyc_np(dom, 31)\n",
        "    dow_sin, dow_cos   = cyc_np(dow, 7)\n",
        "    hour_sin, hour_cos = cyc_np(hour, 24)\n",
        "    min_sin, min_cos   = cyc_np(minute, 60)\n",
        "    sec_sin, sec_cos   = cyc_np(second, 60)\n",
        "\n",
        "    temporal_np = np.column_stack([\n",
        "        years, dom_sin, dom_cos,\n",
        "        dow_sin, dow_cos,\n",
        "        hour_sin, hour_cos,\n",
        "        min_sin, min_cos,\n",
        "        sec_sin, sec_cos\n",
        "    ]).astype(np.float32)\n",
        "\n",
        "    # Combine numeric, flags, temporal\n",
        "    edge_attr_np = np.hstack([num_np_norm, flags_np, temporal_np]).astype(np.float32)\n",
        "\n",
        "    edge_attr = torch.as_tensor(edge_attr_np, dtype=torch.float32)\n",
        "\n",
        "    # Return results\n",
        "    if return_scalers:\n",
        "        return edge_attr, edge_num_mean, edge_num_std\n",
        "    else:\n",
        "        return edge_attr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ojIW0Pa1Au3d"
      },
      "outputs": [],
      "source": [
        "edge_attr_train, edge_num_mean, edge_num_std = build_edge_attr_from_polars(\n",
        "                                              df_train,\n",
        "                                              edge_numeric=edge_numeric,\n",
        "                                              edge_flags=edge_flags,\n",
        "                                              temporal_cols=temporal_cols,\n",
        "                                              return_scalers=True\n",
        "                                          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Y0l4G-hKA9VK"
      },
      "outputs": [],
      "source": [
        "# Compute using the *same normalization and encoding*\n",
        "edge_attr_val = build_edge_attr_from_polars(\n",
        "                                          df_val,\n",
        "                                          edge_numeric=edge_numeric,\n",
        "                                          edge_flags=edge_flags,\n",
        "                                          temporal_cols=temporal_cols,\n",
        "                                          edge_num_mean=edge_num_mean,\n",
        "                                          edge_num_std=edge_num_std\n",
        "                                      )\n",
        "edge_attr_test = build_edge_attr_from_polars(\n",
        "                                          df_test,\n",
        "                                          edge_numeric=edge_numeric,\n",
        "                                          edge_flags=edge_flags,\n",
        "                                          temporal_cols=temporal_cols,\n",
        "                                          edge_num_mean=edge_num_mean,\n",
        "                                          edge_num_std=edge_num_std\n",
        "                                      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "l9d9TBL1BWJS"
      },
      "outputs": [],
      "source": [
        "def build_edge_index_and_labels(df: pl.DataFrame, label_col: str = \"Is_laundering\"):\n",
        "    \"\"\"\n",
        "    Convert Polars DataFrame into edge_index and edge_labels tensors.\n",
        "    Each row = one edge (transaction).\n",
        "    \"\"\"\n",
        "    # Extract edge_index (2 x E)\n",
        "    edge_index_np = df.select([\"src\", \"dst\"]).to_numpy().T     # shape [2, E]\n",
        "    edge_index = torch.as_tensor(edge_index_np, dtype=torch.long)\n",
        "\n",
        "    # Extract edge_labels (E,)\n",
        "    edge_labels_np = df.select(label_col).to_numpy().ravel().astype(\"int64\")\n",
        "    edge_labels = torch.as_tensor(edge_labels_np, dtype=torch.long)\n",
        "\n",
        "    return edge_index, edge_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "4iC-UwkJBh6x"
      },
      "outputs": [],
      "source": [
        "edge_index_train, edge_labels_train = build_edge_index_and_labels(df_train)\n",
        "edge_index_val, edge_labels_val = build_edge_index_and_labels(df_val)\n",
        "edge_index_test, edge_labels_test = build_edge_index_and_labels(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "edge_data = {\n",
        "    'train': {'attr': edge_attr_train, 'index': edge_index_train, 'labels': edge_labels_train},\n",
        "    'val': {'attr': edge_attr_val, 'index': edge_index_val, 'labels': edge_labels_val},\n",
        "    'test': {'attr': edge_attr_test, 'index': edge_index_test, 'labels': edge_labels_test},\n",
        "}\n",
        "\n",
        "for s in ['train', 'val', 'test']:\n",
        "    torch.save(edge_data[s]['attr'], os.path.join(drive_path, f\"edge_attr_{s}.pt\"))\n",
        "    torch.save(edge_data[s]['index'], os.path.join(drive_path, f\"edge_index_{s}.pt\"))\n",
        "    torch.save(edge_data[s]['labels'], os.path.join(drive_path, f\"edge_labels_{s}.pt\"))"
      ],
      "metadata": {
        "id": "mDVO4nuGVBV_"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "od1KHfDIFlju"
      },
      "source": [
        "**Build node features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ei8tZVomFn9n"
      },
      "outputs": [],
      "source": [
        "def build_node_features_from_train(df_train: pl.DataFrame, n_nodes: int,\n",
        "                                   edge_numeric: list, edge_flags: list,\n",
        "                                   node_numeric_agg = [\"mean\",\"sum\",\"std\"], fillna=0.0):\n",
        "    \"\"\"\n",
        "    Produces node_features_df (one row per node id 0..n_nodes-1) and X (torch tensor).\n",
        "    Aggregation uses df_train only (temporal-safe).\n",
        "    \"\"\"\n",
        "    # numeric aggregations: produce mean/sum/std for each numeric col\n",
        "    num_aggs = []\n",
        "    for c in edge_numeric:\n",
        "        num_aggs.append(pl.col(c).mean().alias(f\"{c}_out_mean\"))\n",
        "        num_aggs.append(pl.col(c).sum().alias(f\"{c}_out_sum\"))\n",
        "        num_aggs.append(pl.col(c).std().alias(f\"{c}_out_std\"))\n",
        "    # count of outgoing\n",
        "    num_aggs.append(pl.len().alias(\"out_tx_count\"))\n",
        "\n",
        "    # flag aggregations\n",
        "    flag_aggs = []\n",
        "    for c in edge_flags:\n",
        "        flag_aggs.append(pl.col(c).mean().alias(f\"{c}_out_frac\"))\n",
        "        flag_aggs.append(pl.col(c).max().alias(f\"{c}_out_any\"))\n",
        "\n",
        "    agg = df_train.group_by(\"src\").agg(num_aggs + flag_aggs)\n",
        "\n",
        "    # ensure every node included\n",
        "    nodes_df = pl.DataFrame({\"node\": list(range(n_nodes))})\n",
        "    agg = agg.rename({\"src\": \"node\"})\n",
        "    node_features_df = nodes_df.join(agg, on=\"node\", how=\"left\").fill_null(fillna)\n",
        "\n",
        "    # pick feature columns (exclude 'node'), convert to numpy and torch\n",
        "    node_feature_cols = [c for c in node_features_df.columns if c != \"node\"]\n",
        "    X_np = node_features_df.select(node_feature_cols).to_numpy().astype(np.float32)\n",
        "    # compute node scalers for these columns (train-based)\n",
        "    X_mean = X_np.mean(axis=0)\n",
        "    X_std  = X_np.std(axis=0) + 1e-9\n",
        "    # standardize (optional) - I recommend standardizing node features\n",
        "    X_np = (X_np - X_mean[None, :]) / X_std[None, :]\n",
        "    X = torch.as_tensor(X_np, dtype=torch.float32)\n",
        "    return node_features_df, X, node_feature_cols, X_mean, X_std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "GvmEj3upF3DD"
      },
      "outputs": [],
      "source": [
        "n_nodes = len(mapping_df)\n",
        "node_features_df, X, node_feature_cols, X_mean, X_std = build_node_features_from_train(\n",
        "                                                      df_train, n_nodes, edge_numeric, edge_flags)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "UwM2euvWan3b"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Polars DataFrame as Parquet (efficient + preserves schema)\n",
        "node_features_df.write_parquet(os.path.join(drive_path, \"node_features_df.parquet\"))\n",
        "\n",
        "# Save PyTorch tensor\n",
        "torch.save(X, os.path.join(drive_path, \"X.pt\"))\n",
        "\n",
        "# Save list and NumPy arrays using pickle\n",
        "with open(os.path.join(drive_path, \"node_feature_cols.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(node_feature_cols, f)\n",
        "\n",
        "with open(os.path.join(drive_path, \"X_mean.npy\"), \"wb\") as f:\n",
        "    np.save(f, X_mean)\n",
        "\n",
        "with open(os.path.join(drive_path, \"X_std.npy\"), \"wb\") as f:\n",
        "    np.save(f, X_std)\n"
      ],
      "metadata": {
        "id": "rMNFv0_1VxHh"
      },
      "execution_count": 30,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}